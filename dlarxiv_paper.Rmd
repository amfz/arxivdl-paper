---
title: "dlarxiv"
author: "A Mahfouz"
date: "8/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

In `dlarxiv` we create a package to facilitate downloading subsets of papers from arXiv.

This is important because...

Our package makes use of and complements `aRxiv`

The remainder of this paper is structured as follows: first, we introduce the package's key functions and its supporting functions. Then we provide a longer example of `dlarxiv`'s functionality. Finally, we outline limitations to the current package and future directions for its development.


# Key functions

The main functions in `dlarxiv` are `get_records()` and `download_pdf()`. These two functions form the package's primary workflow of identifying a corpus of papers and then donwloading them from arXiv.

`get_records()` takes as arguments the arXiv categories to query, the time span to search, and the maximum number of records to retrieve. If the function is called in interactive mode (e.g., from the R console), specifying the limit is optional -- the function will call `get_record_count()` and ask the user if they wish to proceed. `get_records()` returns a data frame containing metadata on all papers meeting the criteria, ordered from earliest submission date. An example use case for this is...

`download_pdf()` takes a data frame of arxiv API responses, such as one returned by `get_records()`. It also optionally takes the name of the data frame column containing PDF URLs (`link_col`) and the name of the directory to download papers to (`directory`). If the link column is not specified, the function looks for the "link_pdf" column. If a save directory is not specified, PDFs will be downloaded to the user's current working directory.

# Supporting functions

Supporting functions in `dlarxiv` include `get_record_count()`, which can be used to understand how many papers a query will fetch. `get_record_count()` takes as arguments the arXiv categories to query and the time period over which to search, and returns the number of papers that fit those criteria.

# Vignette

## All papers between certain dates in certain sub-groups
```{r eval=FALSE}
library(dlarxiv)

# check how many papers will be returned
paper_count <- get_record_count("cs.GR|cs.SD", "20200101 TO 20200131")

# get metadata on all sound and graphics papers in January 2020
metadata <- get_records("cs.GR|cs.SD", "20200101 TO 20200131", paper_count)

# download papers to data folder
download_pdf(metadata, directory = "./data")
```

## Using AWS buckets
[support coming soon]

## Something else?

# Next steps and cautions

While the `dlarxiv` package introduces a delay when getting data from arXiv, it does not currently prohibit the user from making large requests of the site. If bulk data on all papers is required, [arXiv's bulk PDF access](https://arxiv.org/help/bulk_data_s3) option may be a better solution.

This package is geared towards facilitating paper downloads by time period and subject area. As such, it does not currently support other types of queries, such as searching for specific authors.